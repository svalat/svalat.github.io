
@inproceedings{dorier_omniscio_2014,
	title = {Omnisc'{IO}: A Grammar-Based Approach to Spatial and Temporal I/O Patterns Prediction},
	doi = {10.1109/SC.2014.56},
	shorttitle = {Omnisc'{IO}},
	abstract = {The increasing gap between the computation performance of post-petascale machines and the performance of their I/O subsystem has motivated many I/O optimizations including prefetching, caching, and scheduling techniques. In order to further improve these techniques, modeling and predicting spatial and temporal I/O patterns of {HPC} applications as they run has became crucial. In this paper we present Omnisc'{IO}, an approach that builds a grammar-based model of the I/O behavior of {HPC} applications and uses it to predict when future I/O operations will occur, and where and how much data will be accessed. Omnisc'{IO} is transparently integrated into the {POSIX} and {MPI} I/O stacks and does not require any modification in applications or higher level I/O libraries. It works without any prior knowledge of the application and converges to accurate predictions within a couple of iterations only. Its implementation is efficient in both computation time and memory footprint.},
	eventtitle = {{SC} '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
	pages = {623--634},
	booktitle = {{SC} '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
	author = {Dorier, M. and Ibrahim, S. and Antoniu, G. and Ross, R.},
	date = {2014-11},
	note = {{ISSN}: 2167-4337},
	keywords = {cache storage, caching techniques, Context, Exascale, Grammar, grammar-based approach, grammars, Hidden Markov models, {HPC}, {HPC} applications, I/O, I/O optimizations, I/O subsystem, input-output programs, Libraries, message passing, {MPI} I/O stacks, Omnisc'{IO}, parallel processing, {POSIX} stacks, post-petascale machines, Prediction, Prediction algorithms, Predictive models, Prefetching, prefetching techniques, scheduling, scheduling techniques, spatial I/O pattern prediction, Storage, storage management, temporal I/O pattern prediction, Unix},
	file = {Version soumise:C\:\\Users\\A708881\\Zotero\\storage\\TJNUXS2W\\Dorier et al. - 2014 - Omnisc'IO A Grammar-Based Approach to Spatial and.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\A708881\\Zotero\\storage\\QYZZIL2R\\7013038.html:text/html},
}

@inproceedings{soumagne_mercury_2013,
	title = {Mercury: Enabling remote procedure call for high-performance computing},
	doi = {10.1109/CLUSTER.2013.6702617},
	shorttitle = {Mercury},
	abstract = {Remote procedure call ({RPC}) is a technique that has been largely adopted by distributed services. This technique, now more and more used in the context of high-performance computing ({HPC}), allows the execution of routines to be delegated to remote nodes, which can be set aside and dedicated to specific tasks. However, existing {RPC} frameworks assume a socket-based network interface (usually on top of {TCP}/{IP}), which is not appropriate for {HPC} systems, because this {API} does not typically map well to the native network transport used on those systems, resulting in lower network performance. In addition, existing {RPC} frameworks often do not support handling large data arguments, such as those found in read or write calls. We present in this paper an asynchronous {RPC} interface, called Mercury, specifically designed for use in {HPC} systems. The interface allows asynchronous transfer of parameters and execution requests and provides direct support of large data arguments. Mercury is generic in order to allow any function call to be shipped. Additionally, the network implementation is abstracted, allowing easy porting to future systems and efficient use of existing native transport mechanisms.},
	eventtitle = {2013 {IEEE} International Conference on Cluster Computing ({CLUSTER})},
	pages = {1--8},
	booktitle = {2013 {IEEE} International Conference on Cluster Computing ({CLUSTER})},
	author = {Soumagne, J. and Kimpe, D. and Zounmevo, J. and Chaarawi, M. and Koziol, Q. and Afsahi, A. and Ross, R.},
	date = {2013-09},
	note = {{ISSN}: 2168-9253},
	keywords = {{HPC}, parallel processing, {API}, application program interfaces, asynchronous {RPC} interface, asynchronous transfer, data handling, Data transfer, distributed services, high-performance computing, large data arguments, Memory management, Mercury, native transport mechanisms, Pipeline processing, Protocols, Registers, remote procedure call, remote procedure calls, Servers, socket-based network interface},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\A708881\\Zotero\\storage\\E8S4S4AY\\6702617.html:text/html},
}

@article{peng_umap_2019,
	title = {{UMap}: Enabling Application-driven Optimizations for Page Management},
	url = {http://arxiv.org/abs/1910.07566},
	shorttitle = {{UMap}},
	abstract = {Leadership supercomputers feature a diversity of storage, from node-local persistent memory and {NVMe} {SSDs} to network-interconnected flash memory and {HDD}. Memory mapping files on different tiers of storage provides a uniform interface in applications. However, system-wide services like mmap are optimized for generality and lack flexibility for enabling application-specific optimizations. In this work, we present Umap to enable user-space page management that can be easily adapted to access patterns in applications and storage characteristics. Umap uses the userfaultfd mechanism to handle page faults in multi-threaded applications efficiently. By providing a data object abstraction layer, Umap is extensible to support various backing stores. The design of Umap supports dynamic load balancing and I/O decoupling for scalable performance. Umap also uses application hints to improve the selection of caching, prefetching, and eviction policies. We evaluate Umap in five benchmarks and real applications on two systems. Our results show that leveraging application knowledge for page management could substantially improve performance. On average, Umap achieved 1.25 to 2.5 times improvement using the adapted configurations compared to the system service.},
	journaltitle = {{arXiv}:1910.07566 [cs]},
	author = {Peng, Ivy B. and {McFadden}, Marty and Green, Eric and Iwabuchi, Keita and Wu, Kai and Li, Dong and Pearce, Roger and Gokhale, Maya},
	urldate = {2021-01-04},
	date = {2019-10-16},
	eprinttype = {arxiv},
	eprint = {1910.07566},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\A708881\\Zotero\\storage\\8IE92RI4\\Peng et al. - 2019 - UMap Enabling Application-driven Optimizations fo.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\A708881\\Zotero\\storage\\GUBFC2Q4\\1910.html:text/html},
}

@inproceedings{essen_di-mmap_2012,
	location = {Salt Lake City, {UT}},
	title = {{DI}-{MMAP}: A High Performance Memory-Map Runtime for Data-Intensive Applications},
	isbn = {978-0-7695-4956-9 978-1-4673-6218-4},
	url = {http://ieeexplore.ieee.org/document/6495881/},
	doi = {10.1109/SC.Companion.2012.99},
	shorttitle = {{DI}-{MMAP}},
	eventtitle = {2012 {SC} Companion: High Performance Computing, Networking, Storage and Analysis ({SCC})},
	pages = {731--735},
	booktitle = {2012 {SC} Companion: High Performance Computing, Networking Storage and Analysis},
	publisher = {{IEEE}},
	author = {Essen, Brian Van and Hsieh, Henry and Ames, Sasha and Gokhale, Maya},
	urldate = {2021-01-04},
	date = {2012-11},
	file = {Version soumise:C\:\\Users\\A708881\\Zotero\\storage\\H2CZXPQ9\\Essen et al. - 2012 - DI-MMAP A High Performance Memory-Map Runtime for.pdf:application/pdf},
}

@inproceedings{vangoor_fuse_2017,
	location = {{USA}},
	title = {To {FUSE} or not to {FUSE}: performance of user-space file systems},
	isbn = {978-1-931971-36-2},
	series = {{FAST}'17},
	shorttitle = {To {FUSE} or not to {FUSE}},
	abstract = {Traditionally, file systems were implemented as part of {OS} kernels. However, as complexity of file systems grew, many new file systems began being developed in user space. Nowadays, user-space file systems are often used to prototype and evaluate new approaches to file system design. Low performance is considered the main disadvantage of user-space file systems but the extent of this problem has never been explored systematically. As a result, the topic of user-space file systems remains rather controversial: while some consider user-space file systems a toy not to be used in production, others develop full-fledged production file systems in user space. In this paper we analyze the design and implementation of the most widely known user-space file system framework--{FUSE}--and characterize its performance for a wide range of workloads. We instrumented {FUSE} to extract useful statistics and traces, which helped us analyze its performance bottlenecks and present our analysis results. Our experiments indicate that depending on the workload and hardware used, performance degradation caused by {FUSE} can be completely imperceptible or as high as -83\% even when optimized; and relative {CPU} utilization can increase by 31\%.},
	pages = {59--72},
	booktitle = {Proceedings of the 15th Usenix Conference on File and Storage Technologies},
	publisher = {{USENIX} Association},
	author = {Vangoor, Bharath Kumar Reddy and Tarasov, Vasily and Zadok, Erez},
	urldate = {2021-01-04},
	date = {2017-02-27},
}

@inproceedings{xue_cprfs_2008,
	location = {New York, {NY}, {USA}},
	title = {{CprFS}: a user-level file system to support consistent file states for checkpoint and restart},
	isbn = {978-1-60558-158-3},
	url = {https://doi.org/10.1145/1375527.1375547},
	doi = {10.1145/1375527.1375547},
	series = {{ICS} '08},
	shorttitle = {{CprFS}},
	abstract = {Checkpoint and Restart ({CPR}) is becoming critical to large scale parallel computers, whose Mean Time Between Failures ({MTBF}) may be much shorter than the execution times of the applications. The {CPR} mechanism should be able to store and recover the states of virtual memory, communication and files for the applications in a consistent way. However, many {CPR} tools ignore file states, which may cause errors for applications with file operations on recovery. Some {CPR} tools adopt library-based approaches or kernel-level file systems to deal with file states, but they only support limited types of file operations which are not sufficient for some applications. Moreover, many library-based approaches are not transparent to user applications because they wrap file {APIs}. Kernel-level file systems are difficult to deploy in production systems due to unnecessary overhead they may introduce to applications that do not need {CPR}. In this paper we propose a user-level file system, {CprFS}, to address these problems. As a file system, {CprFS} can guarantee transparency to user applications, and is convenient to support arbitrary file operations. It can be deployed on applications' demand to avoid intervention with other applications. Experimental results on micro-benchmarks and real-world applications show that {CprFS} introduces acceptable overhead and has little impact on checkpointing systems.},
	pages = {114--123},
	booktitle = {Proceedings of the 22nd annual international conference on Supercomputing},
	publisher = {Association for Computing Machinery},
	author = {Xue, Ruini and Chen, Wenguang and Zheng, Weimin},
	urldate = {2021-01-04},
	date = {2008-06-07},
	keywords = {checkpoint and restart, fault tolerance, file checkpointing, parallel computing},
}

@inproceedings{adam_transparent_2018,
	location = {New York, {NY}, {USA}},
	title = {Transparent High-Speed Network Checkpoint/Restart in {MPI}},
	isbn = {978-1-4503-6492-8},
	url = {https://doi.org/10.1145/3236367.3236383},
	doi = {10.1145/3236367.3236383},
	series = {{EuroMPI}'18},
	abstract = {Fault-tolerance has always been an important topic when it comes to running massively parallel programs at scale. Statistically, hardware and software failures are expected to occur more often on systems gathering millions of computing units. Moreover, the larger jobs are, the more computing hours would be wasted by a crash. In this paper, we describe the work done in our {MPI} runtime to enable transparent checkpointing mechanism. Unlike the {MPI} 4.0 User-Level Failure Mitigation ({ULFM}) interface, our work targets solely Checkpoint/Restart (C/R) and ignores wider features such as resiliency. We show how existing transparent checkpointing methods can be practically applied to {MPI} implementations given a sufficient collaboration from the {MPI} runtime. Our C/R technique is then measured on {MPI} benchmarks such as {IMB} and Lulesh relying on Infiniband high-speed network, demonstrating that the chosen approach is sufficiently general and that performance is mostly preserved. We argue that enabling fault-tolerance without any modification inside target {MPI} applications is possible, and show how it could be the first step for more integrated resiliency combined with failure mitigation like {ULFM}.},
	pages = {1--11},
	booktitle = {Proceedings of the 25th European {MPI} Users' Group Meeting},
	publisher = {Association for Computing Machinery},
	author = {Adam, Julien and Besnard, Jean-Baptiste and Malony, Allen D. and Shende, Sameer and Pérache, Marc and Carribault, Patrick and Jaeger, Julien},
	urldate = {2021-01-04},
	date = {2018-09-23},
	keywords = {Checkpoint-Restart, {DMTCP}, Fault-Tolerance, Infiniband},
}

@inproceedings{cao_transparent_2014,
	location = {New York, {NY}, {USA}},
	title = {Transparent checkpoint-restart over infiniband},
	isbn = {978-1-4503-2749-7},
	url = {https://doi.org/10.1145/2600212.2600219},
	doi = {10.1145/2600212.2600219},
	series = {{HPDC} '14},
	abstract = {Transparently saving the state of the {InfiniBand} network as part of distributed checkpointing has been a long-standing challenge for researchers. The lack of a solution has forced typical {MPI} implementations to include custom checkpoint-restart services that "tear down" the network, checkpoint each node in isolation, and then re-connect the network again. This work presents the first example of transparent, system-initiated checkpoint-restart that directly supports {InfiniBand}. The new approach simplifies current practice by avoiding the need for a privileged kernel module. The generality of this approach is demonstrated by applying it both to {MPI} and to Berkeley {UPC} (Unified Parallel C), in its native mode (without {MPI}). Scalability is shown by checkpointing 2,048 {MPI} processes across 128 nodes (with 16 cores per node). The run-time overhead varies between 0.8\% and 1.7\%. While checkpoint times dominate, the network-only portion of the implementation is shown to require less than 100 milliseconds (not including the time to locally write application memory to stable storage).},
	pages = {13--24},
	booktitle = {Proceedings of the 23rd international symposium on High-performance parallel and distributed computing},
	publisher = {Association for Computing Machinery},
	author = {Cao, Jiajun and Kerr, Gregory and Arya, Kapil and Cooperman, Gene},
	urldate = {2021-01-04},
	date = {2014-06-23},
	keywords = {checkpoint/restart, infiniband, mpi, upc},
	file = {Version soumise:C\:\\Users\\A708881\\Zotero\\storage\\V76TUD8J\\Cao et al. - 2014 - Transparent checkpoint-restart over infiniband.pdf:application/pdf},
}

@inproceedings{tan_extending_2016,
	location = {New York, {NY}, {USA}},
	title = {Extending C++ with co-array semantics},
	isbn = {978-1-4503-4384-8},
	url = {https://doi.org/10.1145/2935323.2935332},
	doi = {10.1145/2935323.2935332},
	series = {{ARRAY} 2016},
	abstract = {The current trend of large scientific computing problems is to align as much as possible to a Single Programming Multiple Data (or {SPMD}) scheme when the application algorithms are conducive to parallelization and vectorization. This reduces the complexity of code because the processors or (computational nodes) perform the same instructions which allows for better performance as algorithms work on local data sets instead of continuously transferring data from one locality to another. However, certain applications, such as stencil problems, demonstrate the need to move data to or from remote localities. This involves an additional degree of complexity, as one must know with which localities to exchange data. In order to solve this issue, Fortran has extended its scalar element indexing approach to distributed structures of elements. In this extension, a structure of scalar elements is attributed a ”co-index” and lives in a specific locality. A co-index provides the application with enough information to retrieve the corresponding data reference. In C++, containers present themselves as a ”smarter” alternative of Fortran arrays but there are still no corresponding standardized features similar to the Fortran co-indexing approach. In this paper, we present an implementation of such features in {HPX}, a general purpose C++ runtime system for applications of any scale. We describe how the combination of the {HPX} features and the actual C++ Standard makes it easy to define a high performance {API} similar to Co-Array Fortran.},
	pages = {63--68},
	booktitle = {Proceedings of the 3rd {ACM} {SIGPLAN} International Workshop on Libraries, Languages, and Compilers for Array Programming},
	publisher = {Association for Computing Machinery},
	author = {Tan, Antoine Tran and Kaiser, Hartmut},
	urldate = {2021-01-04},
	date = {2016-06-02},
	keywords = {{API}, C++, Co-Array, distributed containers, {PGAS}},
	file = {Full Text PDF:C\:\\Users\\A708881\\Zotero\\storage\\LV9GGAUT\\Tan et Kaiser - 2016 - Extending C++ with co-array semantics.pdf:application/pdf},
}

@inproceedings{kalany_efficient_2015,
	location = {New York, {NY}, {USA}},
	title = {Efficient, Optimal {MPI} Datatype Reconstruction for Vector and Index Types},
	isbn = {978-1-4503-3795-3},
	url = {https://doi.org/10.1145/2802658.2802671},
	doi = {10.1145/2802658.2802671},
	series = {{EuroMPI} '15},
	abstract = {Type reconstruction is the process of finding an efficient representation in terms of space and processing time of a data layout as an {MPI} derived datatype. Practically efficient type reconstruction and normalization is important for high-quality {MPI} implementations that strive to provide good performance for communication operations involving noncontiguous data. Although it has recently been shown that the general problem of computing optimal tree representations of derived datatypes allowing any of the {MPI} derived datatype constructors can be solved in polynomial time, the algorithm for this may unfortunately be impractical for datatypes with large counts. By restricting the allowed constructors to vector and index-block type constructors, but excluding the most general {MPI}\_Type\_create\_struct constructor, the problem can be solved much more efficiently. More precisely, we give a new O(n log n/log log n) time algorithm for finding cost-optimal representations of {MPI} type maps of length n using only vector and index-block constructors for a simple but flexible, additive cost model. This improves significantly over a previous O(n√n) time algorithm for the same problem, and the algorithm is simple enough to be considered for practical {MPI} libraries.},
	pages = {1--10},
	booktitle = {Proceedings of the 22nd European {MPI} Users' Group Meeting},
	publisher = {Association for Computing Machinery},
	author = {Kalany, Martin and Träff, Jesper Larsson},
	urldate = {2021-01-04},
	date = {2015-09-21},
}

@inproceedings{perry_improving_2010,
	location = {New York, {NY}, {USA}},
	title = {Improving {MPI} communication via data type fission},
	isbn = {978-1-60558-942-8},
	url = {https://doi.org/10.1145/1851476.1851528},
	doi = {10.1145/1851476.1851528},
	series = {{HPDC} '10},
	abstract = {Message Passing Interface ({MPI}) messages are centered around transmitting instances of {MPI} data types. The data types represented in {MPI} terms are usually modeled after data types native to the application. If a user does not want to transmit a field from the native data type, the user will sometimes align the {MPI} data type such that there is a gap in the displacement where the omitted field would be. With the resulting {MPI} data type now being non-contiguous, cycles are spent making the data contiguous for transmission and then expanding the data back out on the receiving side. We show that by performing data type fission (the process of segregating the transmitted fields from the non-transmitted fields) and aligning the {MPI} data type accordingly, we can completely eliminate the need to copy data during the packing and unpacking process, which can significantly improve the performance of communication-heavy Single Program Multiple Data applications.},
	pages = {352--355},
	booktitle = {Proceedings of the 19th {ACM} International Symposium on High Performance Distributed Computing},
	publisher = {Association for Computing Machinery},
	author = {Perry, Ben and Swany, Martin},
	urldate = {2021-01-04},
	date = {2010-06-21},
	keywords = {communication, data type fission, {MPI}, static transformation},
}

@inproceedings{maheo_optimizing_2014,
	location = {New York, {NY}, {USA}},
	title = {Optimizing Collective Operations in Hybrid Applications},
	isbn = {978-1-4503-2875-3},
	url = {https://doi.org/10.1145/2642769.2642791},
	doi = {10.1145/2642769.2642791},
	series = {{EuroMPI}/{ASIA} '14},
	abstract = {The advent of multicore and manycore processors in clusters advocates for combining {MPI} with a shared memory model like {OpenMP} in high-performance parallel applications. But exploiting hardware resources with such models can be sub optimal. Thus, one approach is to use the hybrid context to perform {MPI} communications. In this paper, we address this issue with a concept of hybrid collective communications, which consists in using {OpenMP} threads to parallelize {MPI} collectives. We validate our approach on several {MPI} libraries ({IntelMPI} and {MPC}), improving the overall time up to a factor of 5.29×, in a real world application.},
	pages = {121--122},
	booktitle = {Proceedings of the 21st European {MPI} Users' Group Meeting},
	publisher = {Association for Computing Machinery},
	author = {Mahéo, Aurèle and Carribault, Patrick and Pérache, Marc and Jalby, William},
	urldate = {2021-01-04},
	date = {2014-09-09},
	keywords = {{MPI}, Collective Communications, {OpenMP}},
}

@article{hoefler_remote_2015,
	title = {Remote Memory Access Programming in {MPI}-3},
	volume = {2},
	issn = {2329-4949},
	url = {https://doi.org/10.1145/2780584},
	doi = {10.1145/2780584},
	abstract = {The Message Passing Interface ({MPI}) 3.0 standard, introduced in September 2012, includes a significant update to the one-sided communication interface, also known as remote memory access ({RMA}). In particular, the interface has been extended to better support popular one-sided and global-address-space parallel programming models to provide better access to hardware performance features and enable new data-access modes. We present the new {RMA} interface and specify formal axiomatic models for data consistency and access semantics. Such models can help users reason about details of the semantics that are hard to extract from the English prose in the standard. It also fosters the development of tools and compilers, enabling them to automatically analyze, optimize, and debug {RMA} programs.},
	pages = {9:1--9:26},
	number = {2},
	journaltitle = {{ACM} Transactions on Parallel Computing},
	shortjournal = {{ACM} Trans. Parallel Comput.},
	author = {Hoefler, Torsten and Dinan, James and Thakur, Rajeev and Barrett, Brian and Balaji, Pavan and Gropp, William and Underwood, Keith},
	urldate = {2021-01-04},
	date = {2015-06-29},
	keywords = {{MPI}, one-sided communication, {RMA}},
}

@inproceedings{holmes_mcmpi_2013,
	location = {New York, {NY}, {USA}},
	title = {{McMPI}: a managed-code {MPI} library in pure C\#},
	isbn = {978-1-4503-1903-4},
	url = {https://doi.org/10.1145/2488551.2488572},
	doi = {10.1145/2488551.2488572},
	series = {{EuroMPI} '13},
	shorttitle = {{McMPI}},
	abstract = {This paper presents {McMPI}, an entirely new {MPI} library written in C\# using only safe managed-code, and performance results from low-level benchmarks demonstrating ping-pong latency and bandwidth comparable with {MS}-{MPI} and {MPICH}2. {McMPI} enables all .Net languages to use {MPI} messaging without introducing a dependency on unsafe non-managed code, e.g. an existing {MPI} library. It also takes advantage of .Net thread support to improve intra-node latency. This paper also discusses support for multiple threads in {McMPI} and proposes an extension to the {MPI} Standard that resolves current ambiguities relating to hosting multiple {MPI} processes in a single operating system process.},
	pages = {25--30},
	booktitle = {Proceedings of the 20th European {MPI} Users' Group Meeting},
	publisher = {Association for Computing Machinery},
	author = {Holmes, Daniel and Booth, Stephen},
	urldate = {2021-01-04},
	date = {2013-09-15},
	keywords = {{MPI}, .Net, C\#, managed-code},
}

@inproceedings{oboyle_integrating_1998,
	location = {{USA}},
	title = {Integrating Loop and Data Transformations for Global Optimisation},
	isbn = {978-0-8186-8591-0},
	series = {{PACT} '98},
	abstract = {This paper is concerned with integrating global data transformations and local loop transformations in order to minimise overhead on distributed shared memory machines such as the {SGi} Origin 2000. By first developing an extended algebraic transformation framework, a new technique to allow the static application of global data transformations, such as partitioning, to reshaped arrays is presented, eliminating the need for expensive temporary copies and hence eliminating any communication and synchronisation. In addition, by integrating loop and data transformations, any introduced poor spatial locality and expensive array subscripts can be eliminated. A specific optimisation algorithm is derived and applied to well-known benchmarks, where it is shown to give a significant improvement in execution time over existing approaches},
	pages = {12},
	booktitle = {Proceedings of the 1998 International Conference on Parallel Architectures and Compilation Techniques},
	publisher = {{IEEE} Computer Society},
	author = {O'Boyle, M. F. P. and Knijnenburg, P. M. W.},
	urldate = {2021-01-04},
	date = {1998-10-12},
	keywords = {Array Reshaping, Data Locality Optimisation, Data Transformations, Global Optimisation, Loop Transformations, Rank-modifying Transformations},
}

@article{song_efficient_2016,
	title = {Efficient Memory-Mapped I/O on Fast Storage Device},
	volume = {12},
	issn = {1553-3077},
	url = {https://doi.org/10.1145/2846100},
	doi = {10.1145/2846100},
	abstract = {In modern operating systems, memory-mapped I/O (mmio) is an important access method that maps a file or file-like resource to a region of memory. The mapping allows applications to access data from files through memory semantics (i.e., load/store) and it provides ease of programming. The number of applications that use mmio are increasing because memory semantics can provide better performance than file semantics (i.e., read/write). As more data are located in the main memory, the performance of applications can be enhanced owing to the effect of a large cache. When mmio is used, hot data tend to reside in the main memory and cold data are located in storage devices such as {HDD} and {SSD}; data placement in the memory hierarchy depends on the virtual memory subsystem of the operating system. Generally, the performance of storage devices has a direct impact on the performance of mmio. It is widely expected that better storage devices will lead to better performance. However, the expectation is limited when fast storage devices are used since the virtual memory subsystem does not reflect the performance feature of those devices. In this article, we examine the Linux virtual memory subsystem and mmio path to determine the influence of fast storage on the existing Linux kernel. Throughout our investigation, we find that the overhead of the Linux virtual memory subsystem, negligible on the {HDD}, prevents applications from using the full performance of fast storage devices. To reduce the overheads and fully exploit the fast storage devices, we present several optimization techniques. We modify the Linux kernel to implement our optimization techniques and evaluate our prototyped system with low-latency storage devices. Experimental results show that our optimized mmio has up to 7x better performance than the original mmio. We also compare our system to a system that has enough memory to keep all data in the main memory. The system with insufficient memory and our mmio achieves 92\% performance of the resource-rich system. This result implies that our virtual memory subsystem for mmap can effectively extend the main memory with fast storage devices.},
	pages = {19:1--19:27},
	number = {4},
	journaltitle = {{ACM} Transactions on Storage},
	shortjournal = {{ACM} Trans. Storage},
	author = {Song, Nae Young and Son, Yongseok and Han, Hyuck and Yeom, Heon Young},
	urldate = {2021-01-04},
	date = {2016-05-20},
	keywords = {data-intensive, Memory-mapped, nonvolatile memory, virtual memory system},
}

@inproceedings{thakur_data_1999,
	title = {Data sieving and collective I/O in {ROMIO}},
	doi = {10.1109/FMPC.1999.750599},
	abstract = {The I/O access patterns of parallel programs often consist of accesses to a large number of small, noncontiguous pieces of data. If an application's I/O needs are met by making many small, distinct I/O requests, however, the I/O performance degrades drastically. To avoid this problem, {MPI}-{IO} allows users to access a noncontiguous data set with a single I/O function call. This feature provides {MPI}-{IO} implementations an opportunity to optimize data access. We describe how our {MPI}-{IO} implementation, {ROMIO}, delivers high performance in the presence of noncontiguous requests. We explain in detail the two key optimizations {ROMIO} performs: data sieving for noncontiguous requests from one process and collective I/O for noncontiguous requests from multiple processes. We describe how one can implement these optimizations portably on multiple machines and file systems, control their memory requirements, and also achieve high performance. We demonstrate the performance and portability with performance results for three applications-an astrophysics-application template ({DIST}3D) the {NAS} {BTIO} benchmark, and an unstructured code ({UNSTRUC})-on five different parallel machines: {HP} Exemplar {IBM} {SP}, Intel Paragon, {NEC} {SX}-4, and {SGI} Origin2000.},
	eventtitle = {Seventh Symposium on the Frontiers of Massively Parallel Computation Proceedings. Frontiers '99},
	pages = {182--189},
	booktitle = {Seventh Symposium on the Frontiers of Massively Parallel Computation Proceedings. Frontiers '99},
	author = {Thakur, R. and Gropp, W. and Lusk, E.},
	date = {1999-02},
	keywords = {parallel processing, astrophysics-application template, Computer science, Control systems, data sieving, {DIST}3D, file systems, File systems, {HP} Exemplar {IBM} {SP}, I/O access patterns, Identity-based encryption, Intel Paragon, Laboratories, Mathematics, {NAS} {BTIO} benchmark, National electric code, {NEC} {SX}-4, noncontiguous data set, optimisation, Parallel machines, parallel programs, performance, performance evaluation, Read only memory, {ROMIO}, {SGI} Origin2000},
	file = {Version soumise:C\:\\Users\\A708881\\Zotero\\storage\\2DKBGIZE\\Thakur et al. - 1999 - Data sieving and collective IO in ROMIO.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\A708881\\Zotero\\storage\\AGEL8KAT\\750599.html:text/html},
}

@inproceedings{kjolstad_automatic_2012,
	location = {New York, {NY}, {USA}},
	title = {Automatic datatype generation and optimization},
	isbn = {978-1-4503-1160-1},
	url = {https://doi.org/10.1145/2145816.2145878},
	doi = {10.1145/2145816.2145878},
	series = {{PPoPP} '12},
	abstract = {Many high performance applications spend considerable time packing noncontiguous data into contiguous communication buffers. {MPI} Datatypes provide an alternative by describing noncontiguous data layouts. This allows sophisticated hardware to retrieve data directly from application data structures. However, packing codes in real-world applications are often complex and specifying equivalent datatypes is difficult, time-consuming, and error prone. We present an algorithm that automates the transformation. We have implemented the algorithm in a tool that transforms packing code to {MPI} Datatypes, and evaluated it by transforming 90 packing codes from the {NAS} Parallel Benchmarks. The transformation allows easy porting of applications to new machines that benefit from datatypes, thus improving programmer productivity.},
	pages = {327--328},
	booktitle = {Proceedings of the 17th {ACM} {SIGPLAN} symposium on Principles and Practice of Parallel Programming},
	publisher = {Association for Computing Machinery},
	author = {Kjolstad, Fredrik and Hoefler, Torsten and Snir, Marc},
	urldate = {2021-01-04},
	date = {2012-02-25},
	keywords = {{MPI}, compiler technique, datatypes, refactoring},
	file = {Texte intégral:C\:\\Users\\A708881\\Zotero\\storage\\P4LF3DYX\\Kjolstad et al. - 2012 - Automatic datatype generation and optimization.pdf:application/pdf},
}

@article{ching_noncontiguous_2002,
	title = {Noncontiguous I/O through {PVFS}},
	url = {http://arxiv.org/abs/cs/0207096},
	abstract = {With the tremendous advances in processor and memory technology, I/O has risen to become the bottleneck in high-performance computing for many applications. The development of parallel file systems has helped to ease the performance gap, but I/O still remains an area needing significant performance improvement. Research has found that noncontiguous I/O access patterns in scientific applications combined with current file system methods to perform these accesses lead to unacceptable performance for large data sets. To enhance performance of noncontiguous I/O we have created list I/O, a native version of noncontiguous I/O. We have used the Parallel Virtual File System ({PVFS}) to implement our ideas. Our research and experimentation shows that list I/O outperforms current noncontiguous I/O access methods in most I/O situations and can substantially enhance the performance of real-world scientific applications.},
	journaltitle = {{arXiv}:cs/0207096},
	author = {Ching, Avery and Choudhary, Alok and Liao, Wei-keng and Ross, Rob and Gropp, William},
	urldate = {2021-01-04},
	date = {2002-07-29},
	eprinttype = {arxiv},
	eprint = {cs/0207096},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, D.4.3},
	file = {arXiv Fulltext PDF:C\:\\Users\\A708881\\Zotero\\storage\\E9DJY5PY\\Ching et al. - 2002 - Noncontiguous IO through PVFS.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\A708881\\Zotero\\storage\\JDUYVK6E\\0207096.html:text/html},
}

@inproceedings{wang_nvmalloc_2012,
	title = {{NVMalloc}: Exposing an Aggregate {SSD} Store as a Memory Partition in Extreme-Scale Machines},
	doi = {10.1109/IPDPS.2012.90},
	shorttitle = {{NVMalloc}},
	abstract = {{DRAM} is a precious resource in extreme-scale machines and is increasingly becoming scarce, mainly due to the growing number of cores per node. On future multi-petaflop and exaflop machines, the memory pressure is likely to be so severe that we need to rethink our memory usage models. Fortunately, the advent of non-volatile memory ({NVM}) offers a unique opportunity in this space. Current {NVM} offerings possess several desirable properties, such as low cost and power efficiency, but suffer from high latency and lifetime issues. We need rich techniques to be able to use them alongside {DRAM}. In this paper, we propose a novel approach for exploiting {NVM} as a secondary memory partition so that applications can explicitly allocate and manipulate memory regions therein. More specifically, we propose an {NVMalloc} library with a suite of services that enables applications to access a distributed {NVM} storage system. We have devised ways within {NVMalloc} so that the storage system, built from compute node-local {NVM} devices, can be accessed in a byte-addressable fashion using the memory mapped I/O interface. Our approach has the potential to re-energize out-of-core computations on large-scale machines by having applications allocate certain variables through {NVMalloc}, thereby increasing the overall memory capacity available. Our evaluation on a 128-core cluster shows that {NVMalloc} enables applications to compute problem sizes larger than the physical memory in a cost-effective manner. It can bring more performance/efficiency gain with increased computation time between {NVM} memory accesses or increased data access locality. In addition, our results suggest that while {NVMalloc} enables transparent access to {NVM}-resident variables, the explicit control it provides is crucial to optimize application performance.},
	eventtitle = {2012 {IEEE} 26th International Parallel and Distributed Processing Symposium},
	pages = {957--968},
	booktitle = {2012 {IEEE} 26th International Parallel and Distributed Processing Symposium},
	author = {Wang, C. and Vazhkudai, S. S. and Ma, X. and Meng, F. and Kim, Y. and Engelmann, C.},
	date = {2012-05},
	note = {{ISSN}: 1530-2075},
	keywords = {Libraries, nonvolatile memory, 128-core cluster, aggregate {SSD} store, Aggregates, byte-addressable fashion, distributed {NVM} storage system, {DRAM}, {DRAM} chips, exaflop machines, extreme-scale machines, Fuses, future multipetaflop machines, memory mapped {IO} interface, memory partition, memory regions, memory usage models, Nonvolatile memory, {NVM}-resident variables, {NVMalloc} library, parallel machines, Performance evaluation, Random access memory, random-access storage, Resource management, secondary memory partition},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\A708881\\Zotero\\storage\\B5EWBIRK\\6267902.html:text/html},
}

@inproceedings{abulila_flatflash_2019,
	location = {New York, {NY}, {USA}},
	title = {{FlatFlash}: Exploiting the Byte-Accessibility of {SSDs} within a Unified Memory-Storage Hierarchy},
	isbn = {978-1-4503-6240-5},
	url = {https://doi.org/10.1145/3297858.3304061},
	doi = {10.1145/3297858.3304061},
	series = {{ASPLOS} '19},
	shorttitle = {{FlatFlash}},
	abstract = {Using flash-based solid state drives ({SSDs}) as main memory has been proposed as a practical solution towards scaling memory capacity for data-intensive applications. However, almost all existing approaches rely on the paging mechanism to move data between {SSDs} and host {DRAM}. This inevitably incurs significant performance overhead and extra I/O traffic. Thanks to the byte-addressability supported by the {PCIe} interconnect and the internal memory in {SSD} controllers, it is feasible to access {SSDs} in both byte and block granularity today. Exploiting the benefits of {SSD}'s byte-accessibility in today's memory-storage hierarchy is, however, challenging as it lacks systems support and abstractions for programs. In this paper, we present {FlatFlash}, an optimized unified memory-storage hierarchy, to efficiently use byte-addressable {SSD} as part of the main memory. We extend the virtual memory management to provide a unified memory interface so that programs can access data across {SSD} and {DRAM} in byte granularity seamlessly. We propose a lightweight, adaptive page promotion mechanism between {SSD} and {DRAM} to gain benefits from both the byte-addressable large {SSD} and fast {DRAM} concurrently and transparently, while avoiding unnecessary page movements. Furthermore, we propose an abstraction of byte-granular data persistence to exploit the persistence nature of {SSDs}, upon which we rethink the design primitives of crash consistency of several representative software systems that require data persistence, such as file systems and databases. Our evaluation with a variety of applications demonstrates that, compared to the current unified memory-storage systems, {FlatFlash} improves the performance for memory-intensive applications by up to 2.3x, reduces the tail latency for latency-critical applications by up to 2.8x, scales the throughput for transactional database by up to 3.0x, and decreases the meta-data persistence overhead for file systems by up to 18.9x. {FlatFlash} also improves the cost-effectiveness by up to 3.8x compared to {DRAM}-only systems, while enhancing the {SSD} lifetime significantly.},
	pages = {971--985},
	booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
	publisher = {Association for Computing Machinery},
	author = {Abulila, Ahmed and Mailthody, Vikram Sharma and Qureshi, Zaid and Huang, Jian and Kim, Nam Sung and Xiong, Jinjun and Hwu, Wen-mei},
	urldate = {2021-01-04},
	date = {2019-04-04},
	keywords = {byte-addressable ssd, data persistence, page promotion, unified memory management},
}

@inproceedings{papagiannis_efficient_2018,
	location = {New York, {NY}, {USA}},
	title = {An Efficient Memory-Mapped Key-Value Store for Flash Storage},
	isbn = {978-1-4503-6011-1},
	url = {https://doi.org/10.1145/3267809.3267824},
	doi = {10.1145/3267809.3267824},
	series = {{SoCC} '18},
	abstract = {Persistent key-value stores have emerged as a main component in the data access path of modern data processing systems. However, they exhibit high {CPU} and I/O overhead. Today, due to power limitations it is important to reduce {CPU} overheads for data processing. In this paper, we propose Kreon, a key-value store that targets servers with flash-based storage, where {CPU} overhead and I/O amplification are more significant bottlenecks compared to I/O randomness. We first observe that two significant sources of overhead in state-of-the-art key-value stores are: (a) The use of compaction in {LSM}-Trees that constantly perform merging and sorting of large data segments and (b) the use of an I/O cache to access devices, which incurs overhead even for data that reside in memory. To avoid these, Kreon performs data movement from level to level by performing partial instead of full data reorganization via the use of a full index per-level. In addition, Kreon uses memory-mapped I/O via a custom kernel path with Copy-On-Write. We implement Kreon as well as our custom memory-mapped I/O path in Linux and we evaluate Kreon using commodity {SSDs} with both small and large datasets (up to 6 billion keys). For a large dataset that stresses I/O, Kreon reduces {CPU} cycles/op by up to 5.8x, reduces I/O amplification for inserts by up to 4.61x, and increases insert ops/s by up to 5.3x, compared to {RocksDB}, a state-of-the-art key-value store that is broadly used today.},
	pages = {490--502},
	booktitle = {Proceedings of the {ACM} Symposium on Cloud Computing},
	publisher = {Association for Computing Machinery},
	author = {Papagiannis, Anastasios and Saloustros, Giorgos and González-Férez, Pilar and Bilas, Angelos},
	urldate = {2021-01-04},
	date = {2018-10-11},
	keywords = {Copy-On-Write, Key-Value Stores, {LSM}-Tree, Memory-Mapped I/O, mmap, {SSD}},
}

@article{jackson_architectures_2018,
	title = {Architectures for High Performance Computing and Data Systems using Byte-Addressable Persistent Memory},
	url = {http://arxiv.org/abs/1805.10041},
	abstract = {Non-volatile, byte addressable, memory technology with performance close to main memory promises to revolutionise computing systems in the near future. Such memory technology provides the potential for extremely large memory regions (i.e. {\textgreater} 3TB per server), very high performance I/O, and new ways of storing and sharing data for applications and workflows. This paper outlines an architecture that has been designed to exploit such memory for High Performance Computing and High Performance Data Analytics systems, along with descriptions of how applications could benefit from such hardware.},
	journaltitle = {{arXiv}:1805.10041 [cs]},
	author = {Jackson, Adrian and Weiland, Michele and Parsons, Mark and Homoelle, Bernhard},
	urldate = {2021-01-04},
	date = {2018-05-25},
	eprinttype = {arxiv},
	eprint = {1805.10041},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Hardware Architecture},
	file = {arXiv.org Snapshot:C\:\\Users\\A708881\\Zotero\\storage\\PR8HVWUG\\1805.html:text/html;Texte intégral:C\:\\Users\\A708881\\Zotero\\storage\\SK8IGYCL\\Jackson et al. - 2018 - Architectures for High Performance Computing and D.pdf:application/pdf},
}

@inproceedings{rudoff_persistent_2017,
	location = {New York, {NY}, {USA}},
	title = {Persistent Memory: The Value to {HPC} and the Challenges},
	isbn = {978-1-4503-5131-7},
	url = {https://doi.org/10.1145/3145617.3158213},
	doi = {10.1145/3145617.3158213},
	series = {{MCHPC}'17},
	shorttitle = {Persistent Memory},
	abstract = {This paper provides an overview of the expected value of emerging persistent memory technologies to high performance computing ({HPC}) use cases. These values are somewhat speculative at the time of writing, based on what has been announced by vendors to become available over the next year, but we describe the potential value to {HPC} as well as some of the challenges in using persistent memory. The enabling work being done in the software ecosystem, applicable to {HPC}, is also described.},
	pages = {7--10},
	booktitle = {Proceedings of the Workshop on Memory Centric Programming for {HPC}},
	publisher = {Association for Computing Machinery},
	author = {Rudoff, Andy},
	urldate = {2021-01-04},
	date = {2017-11-12},
	keywords = {{NVM} programming, persistent memory, storage class memory},
}

@inproceedings{mu_transparent_2018,
	title = {A Transparent Server-Managed Object Storage System for {HPC}},
	doi = {10.1109/CLUSTER.2018.00063},
	abstract = {On the road to exascale, the high-performance computing ({HPC}) community is seeing the emergence of multi-tier storage systems. However, existing data management solutions for {HPC} applications are no longer suitable for handling the increased level of storage complexity and currently delegate that task back to the user. We describe a novel object-based data abstraction that takes advantage of deep memory hierarchies by providing a simplified programming interface that enables autonomous, asynchronous, and transparent data movement with a server-driven architecture. Users can define a mapping between the application memory and abstract storage objects, creating a linkage between either all or part of an object's content without data copy or transfer, avoiding explicit management of complex data movement across multiple storage hierarchies. We evaluate our system by storing plasma physics simulation data with different storage layouts.},
	eventtitle = {2018 {IEEE} International Conference on Cluster Computing ({CLUSTER})},
	pages = {477--481},
	booktitle = {2018 {IEEE} International Conference on Cluster Computing ({CLUSTER})},
	author = {Mu, J. and Soumagne, J. and Tang, H. and Byna, S. and Koziol, Q. and Warren, R.},
	date = {2018-09},
	note = {{ISSN}: 2168-9253},
	keywords = {{HPC} applications, parallel processing, storage management, application program interfaces, asynchronous transfer, data handling, high-performance computing, Memory management, Servers, abstract storage objects, application memory, asynchronous, data movement, complex data movement, Complexity theory, Containers, data management solutions, deep memory hierarchies, explicit management, large scale systems, memory management, Metadata, multitier storage systems, object-based data abstraction, object-centric models, Semantics, server-driven architecture, simplified programming interface, storage complexity, storage hierarchies, storage layouts, transparent data movement, transparent server-managed object storage system},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\A708881\\Zotero\\storage\\7CZCDVAU\\8514907.html:text/html},
}

@inproceedings{tang_toward_2018,
	title = {Toward Scalable and Asynchronous Object-Centric Data Management for {HPC}},
	doi = {10.1109/CCGRID.2018.00026},
	abstract = {Emerging high performance computing ({HPC}) systems are expected to be deployed with an unprecedented level of complexity due to a deep system memory and storage hierarchy. Efficient and scalable methods of data management and movement through this hierarchy is critical for scientific applications using exascale systems. Moving toward new paradigms for scalable I/O in the extreme-scale era, we introduce novel object-centric data abstractions and storage mechanisms that take advantage of the deep storage hierarchy, named Proactive Data Containers ({PDC}). In this paper, we formulate object-centric {PDCs} and their mappings in different levels of the storage hierarchy. {PDC} adopts a client-server architecture with a set of servers managing data movement across storage layers. To demonstrate the effectiveness of the proposed {PDC} system, we have measured performance of benchmarks and I/O kernels from scientific simulation and analysis applications using {PDC} programming interface, and compared the results with existing highly tuned I/O libraries. Using asynchronous I/O along with data and metadata optimizations, {PDC} demonstrates up to 23× speedup over {HDF}5 and {PLFS} in writing and reading data from a plasma physics simulation. {PDC} achieves comparable performance with {HDF}5 and {PLFS} in reading and writing data of a single timestep at small scale, and outperforms them at a scale of larger than 10K cores. In contrast to existing storage systems, {PDC} offers user-space data management with the flexibility to allocate the number of {PDC} servers depending on the workload.},
	eventtitle = {2018 18th {IEEE}/{ACM} International Symposium on Cluster, Cloud and Grid Computing ({CCGRID})},
	pages = {113--122},
	booktitle = {2018 18th {IEEE}/{ACM} International Symposium on Cluster, Cloud and Grid Computing ({CCGRID})},
	author = {Tang, H. and Byna, S. and Tessier, F. and Wang, T. and Dong, B. and Mu, J. and Koziol, Q. and Soumagne, J. and Vishwanath, V. and Liu, J. and Warren, R.},
	date = {2018-05},
	keywords = {{HPC}, parallel processing, storage management, application program interfaces, Servers, Containers, Metadata, asynchronous, asynchronous object-centric data management, client-server architecture, client-server systems, Computer architecture, data management, data movement, data structures, deep storage hierarchy, deep system memory, Handheld computers, {HDF}5, high-performance computing systems, meta data, object centric, object-centric data abstractions, object-centric {PDC}, Optimization, {PDC} programming interface, {PDC} servers, proactive data containers, Programming, scalable, storage layers, storage mechanisms, storage systems, user-space data management},
	file = {Version soumise:C\:\\Users\\A708881\\Zotero\\storage\\P7CM35FN\\Tang et al. - 2018 - Toward Scalable and Asynchronous Object-Centric Da.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\A708881\\Zotero\\storage\\EEVT7L4V\\8411015.html:text/html},
}

@inproceedings{tang_someta_2017,
	title = {{SoMeta}: Scalable Object-Centric Metadata Management for High Performance Computing},
	doi = {10.1109/CLUSTER.2017.53},
	shorttitle = {{SoMeta}},
	abstract = {Scientific data sets, which grow rapidly in volume, are often attached with plentiful metadata, such as their associated experiment or simulation information. Thus, it becomes difficult for them to be utilized and their value is lost over time. Ideally, metadata should be managed along with its corresponding data by a single storage system, and can be accessed and updated directly. However, existing storage systems in high-performance computing ({HPC}) environments, such as Lustre parallel file system, still use a static metadata structure composed of non-extensible and fixed amount of information. The burden of metadata management falls upon the end-users and require ad-hoc metadata management software to be developed.With the advent of "object-centric" storage systems, there is an opportunity to solve this issue. In this paper, we present {SoMeta}, a scalable and decentralized metadata management approach for object-centric storage in {HPC} systems. It provides a flat namespace that is dynamically partitioned, a tagging approach to manage metadata that can be efficiently searched and updated, and a light-weight and fault tolerant management strategy. In our experiments, {SoMeta} achieves up to 3.7X speedup over Lustre in performing common metadata operations, and up to 16X faster than {SciDB} and {MongoDB} for advanced metadata operations, such as adding and searching tags. Additionally, in contrast to existing storage systems, {SoMeta} offers scalable user-space metadata management by allowing users with the capability to specify the number of metadata servers depending on their workload.},
	eventtitle = {2017 {IEEE} International Conference on Cluster Computing ({CLUSTER})},
	pages = {359--369},
	booktitle = {2017 {IEEE} International Conference on Cluster Computing ({CLUSTER})},
	author = {Tang, H. and Byna, S. and Dong, B. and Liu, J. and Koziol, Q.},
	date = {2017-09},
	note = {{ISSN}: 2168-9253},
	keywords = {parallel processing, Servers, Metadata, meta data, scalable, ad-hoc metadata management software, decentralized metadata management approach, Fault tolerance, fault tolerant management strategy, Fault tolerant systems, high performance computing, high-performance computing environments, {HPC} systems, Lustre parallel file system, metadata management, metadata operations, metadata servers, {MongoDB}, object-centric, object-centric storage systems, Scalability, scalable metadata management approach, scalable object-centric metadata management, {SciDB}, scientific data sets, Search problems, {SoMeta} approach, static metadata structure, storage system, Tagging, user-space metadata management},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\A708881\\Zotero\\storage\\3L4L482R\\8048948.html:text/html},
}

@inproceedings{basu_efficient_2013,
	location = {New York, {NY}, {USA}},
	title = {Efficient virtual memory for big memory servers},
	isbn = {978-1-4503-2079-5},
	url = {https://doi.org/10.1145/2485922.2485943},
	doi = {10.1145/2485922.2485943},
	series = {{ISCA} '13},
	abstract = {Our analysis shows that many "big-memory" server workloads, such as databases, in-memory caches, and graph analytics, pay a high cost for page-based virtual memory. They consume as much as 10\% of execution cycles on {TLB} misses, even using large pages. On the other hand, we find that these workloads use read-write permission on most pages, are provisioned not to swap, and rarely benefit from the full flexibility of page-based virtual memory. To remove the {TLB} miss overhead for big-memory workloads, we propose mapping part of a process's linear virtual address space with a direct segment, while page mapping the rest of the virtual address space. Direct segments use minimal hardware---base, limit and offset registers per core---to map contiguous virtual memory regions directly to contiguous physical memory. They eliminate the possibility of {TLB} misses for key data structures such as database buffer pools and in-memory key-value stores. Memory mapped by a direct segment may be converted back to paging when needed. We prototype direct-segment software support for x86-64 in Linux and emulate direct-segment hardware. For our workloads, direct segments eliminate almost all {TLB} misses and reduce the execution time wasted on {TLB} misses to less than 0.5\%.},
	pages = {237--248},
	booktitle = {Proceedings of the 40th Annual International Symposium on Computer Architecture},
	publisher = {Association for Computing Machinery},
	author = {Basu, Arkaprava and Gandhi, Jayneel and Chang, Jichuan and Hill, Mark D. and Swift, Michael M.},
	urldate = {2021-02-02},
	date = {2013-06-23},
	keywords = {tanslation lookaside buffer, virtual memory},
	file = {Full Text PDF:C\:\\Users\\A708881\\Zotero\\storage\\K3XE9R7Y\\Basu et al. - 2013 - Efficient virtual memory for big memory servers.pdf:application/pdf},
}

@inproceedings{valat_numaprof_2019,
	location = {Cham},
	title = {{NUMAPROF}, A {NUMA} Memory Profiler},
	isbn = {978-3-030-10549-5},
	doi = {10.1007/978-3-030-10549-5_13},
	series = {Lecture Notes in Computer Science},
	abstract = {The number of cores in {HPC} systems and servers increased a lot for the last few years. In order to also increase the available memory bandwidth and capacity, most systems became {NUMA} (Non-Uniform Memory Access) meaning each processor has its own memory and can share it. Although the access to the remote memory is transparent for the developer, it comes with a lower bandwidth and a higher latency. It might heavily impact the performance of the application if it happens too often. Handling this memory locality in multi-threaded applications is a challenging task. In order to help the developer, we developed {NUMAPROF}, a memory profiling tool pinpointing the local and remote memory accesses onto the source code with the same approach as {MALT}, a memory allocation profiling tool. The paper offers a full review of the capacity of {NUMAPROF} on mainstream {HPC} workloads. In addition to the dedicated interface, the tool also provides hints about unpinned memory accesses (unpinned thread or unpinned page) which can help the developer find portion of codes not safely handling the {NUMA} binding. The tool also provides dedicated metrics to track access to {MCDRAM} of the Intel Xeon Phi codenamed Knight’s Landing. To operate, the tool instruments the application by using Pin, a parallel binary instrumentation framework from Intel. {NUMAPROF} also has the particularity of using the {OS} memory mapping without relying on hardware counters or {OS} simulation. It permits understanding what really happened on the system without requiring dedicated hardware support.},
	pages = {159--170},
	booktitle = {Euro-Par 2018: Parallel Processing Workshops},
	publisher = {Springer International Publishing},
	author = {Valat, Sébastien and Bouizi, Othman},
	editor = {Mencagli, Gabriele and B. Heras, Dora and Cardellini, Valeria and Casalicchio, Emiliano and Jeannot, Emmanuel and Wolf, Felix and Salis, Antonio and Schifanella, Claudio and Manumachu, Ravi Reddy and Ricci, Laura and Beccuti, Marco and Antonelli, Laura and Garcia Sanchez, José Daniel and Scott, Stephen L.},
	date = {2019},
	langid = {english},
	keywords = {Access, Instrumentation, {KNL}, {MCDRAM}, Memory, {NUMA}, Pin, Profiler, Remote},
}

@inproceedings{valat_malt_2017,
	location = {New York, {NY}, {USA}},
	title = {{MALT}: a Malloc tracker},
	isbn = {978-1-4503-5517-9},
	url = {https://doi.org/10.1145/3141865.3141867},
	doi = {10.1145/3141865.3141867},
	series = {{SEPS} 2017},
	shorttitle = {{MALT}},
	abstract = {At the beginning of computer science memory management was a big issue with applications requiring to fit in the small amount of available memory (close to a few kilobytes). Hardware evolution has made this resource cheap for the past few years. Available memory is now close to a few hundred gigabytes. But the current evolution in the multi/many-core era tends to make some issues come back. The memory available tends not to follow the increasing number of cores making the memory resource per thread rare again. We also encounter new issues with the requirement to manage a bigger space with many more allocated objects. This new aspect increases the probability of memory leaks. It also increases the probability of memory management performance issues. Hence, with {MALT} we provide a tool to track the memory allocated by an application. We then map the extracted metrics onto the source code, just like kcachegrind does with valgrind for the {CPU} performance. Compared to most available tools, {MALT} can also be used to track potential performance losses due to bad allocation patterns (too many allocations, small allocations, recycling large allocations, short-lived allocations...) thanks to the various metrics it exposes to the user. This paper will detail the metrics extracted by {MALT} and how we present them to the user thanks to a nice web based graphical interface which is missing with most of the available Linux tools.},
	pages = {1--10},
	booktitle = {Proceedings of the 4th {ACM} {SIGPLAN} International Workshop on Software Engineering for Parallel Systems},
	publisher = {Association for Computing Machinery},
	author = {Valat, Sébastien and Charif-Rubial, Andres S. and Jalby, William},
	urldate = {2021-02-02},
	date = {2017-10-23},
	keywords = {allocation, management, memory, profiling, tool},
	file = {Full Text PDF:C\:\\Users\\A708881\\Zotero\\storage\\DR66G6Q6\\Valat et al. - 2017 - MALT a Malloc tracker.pdf:application/pdf},
}

@thesis{valat_contribution_2014,
	title = {Contribution à l'amélioration des méthodes d'optimisation de la gestion de la mémoire dans le cadre du Calcul Haute Performance},
	url = {https://hal.archives-ouvertes.fr/tel-01253537},
	institution = {University de Versaille Saint-Quentin en Yvelines},
	type = {Theses},
	author = {Valat, Sébastien Jean},
	urldate = {2021-02-02},
	date = {2014-07},
	keywords = {{HPC}, optimisation, {NUMA}, memory, allocateur, allocator, caches, hierarchie, linux, malloc, mémoire, multi-coeurs, operating system, optimization, {OS}, parallel, supercalculateur, système d'exploitation, threads},
	file = {HAL PDF Full Text:C\:\\Users\\A708881\\Zotero\\storage\\ZYBAXBMH\\Valat - 2014 - Contribution à l'amélioration des méthodes d'optim.pdf:application/pdf},
}

@inproceedings{valat_introducing_2013,
	location = {New York, {NY}, {USA}},
	title = {Introducing kernel-level page reuse for high performance computing},
	isbn = {978-1-4503-2103-7},
	url = {https://doi.org/10.1145/2492408.2492414},
	doi = {10.1145/2492408.2492414},
	series = {{MSPC} '13},
	abstract = {Due to computer architecture evolution, more and more {HPC} applications have to include thread-based parallelism and take care of memory consumption. Such evolutions require more attention to the full memory management chain, particularly stressed in multi-threaded context. Several memory allocators provide better scalability on the user-space side. But, with the steadily increasing number of cores, the impact of the operating system cannot be neglected anymore. We measured performance impact of the {OS} memory sub-system for up to one third of the total execution time of a real application on 128 cores. On modern architectures, we measured that up to 40\% of the page fault time is spent in page zeroing. In this paper, we detail a proposal to improve paging performance by removing the needs of this unproductive page zeroing through an extension of the mmap semantic. To this end, we added a kernel-level memory page pool per process to locally reuse free pages without content reset. Our experiments show significant performance improvements especially for huge pages.},
	pages = {1--9},
	booktitle = {Proceedings of the {ACM} {SIGPLAN} Workshop on Memory Systems Performance and Correctness},
	publisher = {Association for Computing Machinery},
	author = {Valat, Sébastien and Pérache, Marc and Jalby, William},
	urldate = {2021-02-02},
	date = {2013-06-16},
	keywords = {{NUMA}, parallel, kernel, Linux, many-core, memory allocator, memory pool, page fault, process, zero page},
	file = {Full Text PDF:C\:\\Users\\A708881\\Zotero\\storage\\WEU57P5T\\Valat et al. - 2013 - Introducing kernel-level page reuse for high perfo.pdf:application/pdf},
}

@article{colombo_flit-level_2019,
	title = {Flit-Level {InfiniBand} Network Simulations of the {DAQ} System of the {LHCb} Experiment for Run-3},
	volume = {66},
	issn = {1558-1578},
	doi = {10.1109/TNS.2019.2905993},
	abstract = {The Large Hadron Collider beauty ({LHCb}) experiment is designed to study the differences between particles and antiparticles as well as very rare decays in the charm and beauty sector at the ({LHC}). The detector will be upgraded in 2019, and a new trigger-less readout system will be implemented in order to significantly increase its efficiency and fully take advantage of the provided machine luminosity at the {LHCb} collision point. In the upgraded system, both event building and event filtering will be performed in software for all the data produced in every bunch-crossing of the {LHC}. In order to transport the full data rate of 32 Tb/s, we will use custom field-programmable gate array ({FPGA}) readout boards ({PCIe}40) and the state-of-the-art off-the-shelf network technologies. The full-event-building system will require around 500 servers interconnected together. From a networking point of view, event building traffic has an all-to-all pattern, requiring careful design of the network architecture to avoid congestion at the data rates foreseen. In order to maximize link utilization, different techniques can be adopted in various areas like traffic shaping, network topology, and routing optimization. The size of the system makes it very difficult to test at production scale, before the actual procurement. We resort, therefore, to network simulations as a powerful tool for finding the optimal configuration. We will present an accurate low-level description of an {InfiniBand}-based network with event building like traffic. We will show a comparison between simulated and reduced scale systems and how changes in the input parameters affect the performance.},
	pages = {1159--1164},
	number = {7},
	journaltitle = {{IEEE} Transactions on Nuclear Science},
	author = {Colombo, T. and Durante, P. and Galli, D. and Manzali, M. and Marconi, U. and Neufeld, N. and Pisani, F. and Schwemmer, R. and Valat, S.},
	date = {2019-07},
	note = {Conference Name: {IEEE} Transactions on Nuclear Science},
	keywords = {Computer architecture, antiparticles, Architecture, beauty sector, Buildings, bunch-crossing, charm sector, custom field-programmable gate array readout boards, {DAQ} system, data acquisition, Data acquisition, Data acquisition systems, data rate, event building traffic, event filtering, field programmable gate arrays, flit-level {InfiniBand} network simulations, full-event-building system, high energy physics instrumentation computing, {InfiniBand}-based network, Large Hadron Collider, Large Hadron Collider beauty experiment, {LHCb} collision point, {LHCb} experiment, low-level description, network architecture, network topology, Network topology, networks, {PCIe}40, provided machine luminosity, rare decays, readout electronics, simulated reduced scale systems, simulation, state-of-the-art off-the-shelf network technologies, Switches, trigger circuits, trigger-less readout system, upgraded system},
	file = {Version soumise:C\:\\Users\\A708881\\Zotero\\storage\\RCGWGEVH\\Colombo et al. - 2019 - Flit-Level InfiniBand Network Simulations of the D.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\A708881\\Zotero\\storage\\ZUTXPXE9\\8668802.html:text/html},
}

@article{baymani_exploring_2017,
	title = {Exploring {RapidIO} Technology Within a {DAQ} System Event Building Network},
	volume = {64},
	issn = {1558-1578},
	doi = {10.1109/TNS.2017.2734564},
	abstract = {{RapidIO} (http://rapidio.org/) technology is a packet-switched high-performance fabric, which has been under active development since 1997. The technology is used in all 4G/{LTE} base stations worldwide. {RapidIO} is also used in embedded systems that require high reliability, low latency, and deterministic operations in a heterogeneous environment. {RapidIO} has several offloading features in hardware, therefore relieving the {CPUs} from time- and power-consuming work. Most importantly, it allows for remote direct memory access and thus zero-copy data transfer. In addition, it lends itself readily to integration with field-programmable gate arrays. In this paper, we investigate {RapidIO} as a technology for high-speed data acquisition ({DAQ}) networks, in particular the {DAQ} system of an {LHC} experiment. We present measurements using a generic multiprotocol event-building emulation tool that was developed for the {LHCb} experiment. Event building using a local area network, such as the one foreseen for the future {LHCb} {DAQ}, puts heavy requirements on the underlying network as all data sources from the collider will want to send to the same destinations at the same time. This may lead to an instantaneous over commitment of the output buffers of the switches. We will present results from implementing an event building cluster based on {RapidIO} interconnect, focusing on the bandwidth capabilities of the technology as well as its scalability.},
	pages = {2598--2605},
	number = {9},
	journaltitle = {{IEEE} Transactions on Nuclear Science},
	author = {Baymani, S. and Alexopoulos, K. and Valat, S.},
	date = {2017-09},
	note = {Conference Name: {IEEE} Transactions on Nuclear Science},
	keywords = {Libraries, Protocols, data acquisition, Data acquisition, {LHCb} experiment, bandwidth capabilities, Communication systems, {DAQ} networks, {DAQ} system event building network, data acquisition-protocol independent performance evaluator ({DAQPIPE}), data analysis, Data analysis, deterministic operations, embedded systems, Embedded systems, event building cluster, Fabrics, field-programmable gate arrays, Hardware, heterogeneous environment, high-speed data acquisition networks, interconnected systems, offloading features, packet-switched high-performance fabric, {RapidIO}, {RapidIO} interconnect, {RapidIO} technology, remote direct memory access, {ROOT}, scalability, zero-copy data transfer},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\A708881\\Zotero\\storage\\UUSCKLN4\\Baymani et al. - 2017 - Exploring RapidIO Technology Within a DAQ System E.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\A708881\\Zotero\\storage\\FJGKCBKF\\7999252.html:text/html},
}

@article{valat_evaluation_2017,
	title = {An Evaluation of 100-Gb/s {LAN} Networks for the {LHCb} {DAQ} Upgrade},
	volume = {64},
	issn = {1558-1578},
	doi = {10.1109/TNS.2017.2687124},
	abstract = {The Large Hadron Collider Beauty experiment ({LHCb}) experiment is preparing a major upgrade resulting in the need for a high-end network for the data acquisition system. Its capacity will grow up to a target speed of 40 Tb/s, aggregated by 500 nodes. This can only be achieved reasonably by using links that are capable of coping with 100-Gb/s line rates. The constantly increasing need for more and more bandwidth has initiated the development of commercial 100-Gb/s networks. There are three candidates on the horizon that need to be considered: Intel Omni-Path, 100-G Ethernet, and {EDR} {InfiniBand}. We present test results with such links using both standard benchmarks (e.g., iperf) and a custom benchmark called Data {AcQquisition} ({DAQ}) Protocol Independent Performance Evaluator ({DAQPIPE}). With {DAQPIPE}, we mainly evaluate the ability to exploit the targeted network for a kind of all-to-all communication pattern. The key benefit of these measurements is that it helps us to tune our benchmark and improves our understanding of the relevant parameters. It will now permit us to prepare and motivate some upcoming tests at scale on existing supercomputers offering the targeted hardware.},
	pages = {1480--1485},
	number = {6},
	journaltitle = {{IEEE} Transactions on Nuclear Science},
	author = {Valat, S. and Vőneki, B. and Neufeld, N. and Machen, J. and Schwemmer, R. and Pérez, D. H. Cámpora},
	date = {2017-06},
	note = {Conference Name: {IEEE} Transactions on Nuclear Science},
	keywords = {Infiniband, {MPI}, Buildings, data acquisition, {LHCb} experiment, Hardware, 100-G Ethernet, all-to-all communication pattern, Bandwidth, Benchmark, Benchmark testing, bit rate 100 Gbit/s, {DAQ} protocol independent performance evaluator, {DAQPIPE}, data acquisition system, detector, Detectors, {EDR} {InfiniBand}, Ethernet, hadrons, high-end network, high-performance computing ({HPC}), Intel Omni-Path, Kernel, {LAN} networks, large hadron collider beauty experiment, {LHCb} {DAQ} upgrade, local area networks, networkds, Omni-Path, protocols, standard benchmarks, Standards, supercomputers},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\A708881\\Zotero\\storage\\JML5P393\\7886309.html:text/html},
}

@article{manzali_large-scale_2017,
	title = {Large-Scale {DAQ} Tests for the {LHCb} Upgrade},
	volume = {64},
	issn = {1558-1578},
	doi = {10.1109/TNS.2017.2693422},
	abstract = {To increase the event yield, the {LHCb} experiment will undergo a major detector upgrade planned during the second long shutdown of the Large Hadron Collider (2019-2020). The new data acquisition has to process the whole 40-{MHz} input event rate, relying only on the large-scale computing farm implementation of the high-level trigger. Event fragments will be forwarded at 40 {MHz} from the detector front-end electronics to the event builder ({EB}), through optical links and peripheral component interconnect express cards. The {EB} farm, of about 500 computers, shall provide an aggregated throughput of 32 Tb/s. To reach the required {EB} performance, we are testing various interconnect technologies and network protocols on large-scale computing clusters. For this purpose, we have developed an {EB} software evaluator. We report here about the results of the measurements performed on high-performance computing facilities to test throughput and scalability.},
	pages = {1486--1493},
	number = {6},
	journaltitle = {{IEEE} Transactions on Nuclear Science},
	author = {Manzali, M. and Falabella, A. and Giacomini, F. and Marconi, U. and Neufeld, N. and Valat, S. and Voneki, B.},
	date = {2017-06},
	note = {Conference Name: {IEEE} Transactions on Nuclear Science},
	keywords = {Protocols, Data acquisition, high energy physics instrumentation computing, Large Hadron Collider, {LHCb} experiment, readout electronics, Hardware, Bandwidth, Detectors, Clocks, Data acquisition ({DAQ}), {EB} software evaluator, event building, high-level trigger, high-throughput computing, {infiniBand}, large-scale {DAQ} tests, {LHCb}, {LHCb} Upgrade, Software},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\A708881\\Zotero\\storage\\F6YXAQK3\\7898441.html:text/html},
}

@inproceedings{cesini_high_2016,
	title = {High throughput data acquisition with {InfiniBand} on x86 low-power architectures for the {LHCb} upgrade},
	doi = {10.1109/RTC.2016.7543136},
	abstract = {The {LHCb} Collaboration is preparing a major upgrade of the detector and the Data Acquisition ({DAQ}) to be installed during the {LHC}-{LS}2. The new Event Builder computing farm for the {DAQ} requires about 500 nodes, and have to be capable of transporting order of 32 Tbps. The requested performance can possibly be achieved using high-bandwidth data-centre switches and commodity hardware. Several studies are ongoing to evaluate and compare network and hardware technologies, with the aim of optimising the performance and also the purchase and maintenance costs of the system. We are investigating if x86 low-power architectures can achieve equivalent performance as traditional servers when used for multi gigabit {DAQ}. In this talk we introduce an Event Builder implementation based on {InfiniBand} network and show preliminary tests with this network technology on x86 low-power architectures, such as Intel Atom C2750 and Intel Xeon D-1540, comparing measured bandwidth and power consumption.},
	eventtitle = {2016 {IEEE}-{NPSS} Real Time Conference ({RT})},
	pages = {1--3},
	booktitle = {2016 {IEEE}-{NPSS} Real Time Conference ({RT})},
	author = {Cesini, D. and Ferraro, A. and Falabella, A. and Giacomini, F. and Manzali, M. and Marconi, U. and Neufeld, N. and Valat, S. and Voneki, B.},
	date = {2016-06},
	keywords = {Servers, Computer architecture, data acquisition, Data acquisition, Bandwidth, Atomic measurements, Event Builder computing, hardware technologies, high-bandwidth data-centre switches, {InfiniBand} network, Intel Atom C2750, Intel Xeon D-1540, {LHC}-{LS}2, {LHCb} upgrade, multi gigabit {DAQ}, Power demand, Program processors},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\A708881\\Zotero\\storage\\WQMUFXNU\\7543136.html:text/html},
}

@online{noauthor_regards_nodate,
	title = {Regards d'actualité au prisme des enjeux sociétaux sur les données historisées d'{EGC} - Editions {RNTI}},
	rights = {All rights reserved},
	url = {https://editions-rnti.fr/?inprocid=1002574},
	urldate = {2021-02-02},
	file = {Regards dactualité au prisme des enjeux sociétaux sur les données historisées dEGC - Editions RNTI:C\:\\Users\\A708881\\Zotero\\storage\\QRB6FSTQ\\editions-rnti.fr.html:text/html},
}

@inproceedings{bautista-gomez_fti_2011,
	title = {{FTI}: High performance Fault Tolerance Interface for hybrid systems},
	doi = {10.1145/2063384.2063427},
	shorttitle = {{FTI}},
	abstract = {Large scientific applications deployed on current petascale systems expend a significant amount of their execution time dumping checkpoint files to remote storage. New fault tolerant techniques will be critical to efficiently exploit post-petascale systems. In this work, we propose a low-overhead high-frequency multi-level checkpoint technique in which we integrate a highly-reliable topology-aware Reed-Solomon encoding in a three-level checkpoint scheme. We efficiently hide the encoding time using one Fault-Tolerance dedicated thread per node. We implement our technique in the Fault Tolerance Interface {FTI}. We evaluate the correctness of our performance model and conduct a study of the reliability of our library. To demonstrate the performance of {FTI}, we present a case study of the Mw9.0 Tohoku Japan earthquake simulation with {SPECFEM}3D on {TSUBAME}2.0. We demonstrate a checkpoint overhead as low as 8\% on sustained 0.1 petaflops runs (1152 {GPUs}) while check-pointing at high frequency.},
	eventtitle = {{SC} '11: Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis},
	pages = {1--12},
	booktitle = {{SC} '11: Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis},
	author = {Bautista-Gomez, L. and Tsuboi, S. and Komatitsch, D. and Cappello, F. and Maruyama, N. and Matsuoka, S.},
	date = {2011-11},
	note = {{ISSN}: 2167-4337},
	keywords = {Libraries, Fault tolerance, Fault tolerant systems, checkpointing, Computational modeling, earthquakes, Encoding, fault tolerance interface, fault tolerant computing, geophysics computing, graphics processing units, hybrid system, low-overhead high-frequency multilevel checkpoint technique, mainframes, Mw9.0 Tohoku Japan earthquake simulation, petascale system, Reed-Solomon codes, {SPECFEM}3D, three-level checkpoint scheme, topology, topology-aware Reed-Solomon encoding, Tsubame2.0, user interfaces, Writing},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\A708881\\Zotero\\storage\\E6PV8GMQ\\6114441.html:text/html;Bautista-Gomez et al. - 2011 - FTI High performance Fault Tolerance Interface fo.pdf:C\:\\Users\\A708881\\Zotero\\storage\\D5YJRGNL\\Bautista-Gomez et al. - 2011 - FTI High performance Fault Tolerance Interface fo.pdf:application/pdf},
}

@inproceedings{goglin_opportunities_2019,
	title = {Opportunities for Partitioning Non-Volatile Memory {DIMMs} between Co-scheduled Jobs on {HPC} Nodes},
	url = {https://hal.inria.fr/hal-02173336},
	abstract = {The emergence of non-volatile memory {DIMMs} such as Intel Optane {DCPMM} blurs the gap between usual volatile memory and persistent storage by enabling byte-accessible persistent memory with reasonable performance. This new hardware supports many possible use cases for high-performance applications, from high performance storage to very high-capacity volatile memory (terabytes). However the numerous ways to configure the memory subsystem raises the question of how to configure nodes to satisfy applications' needs (memory, storage, fault tolerance, etc.). We focus on the issue of partitioning {HPC} nodes with {NVDIMMs} in the context of co-scheduling multiple jobs. We show that the basic {NVDIMM} configuration modes would require node reboots and expensive hardware configuration. Moreover it does not allow the co-scheduling of all kinds of jobs, and it does not always allow locality to be taken into account during resource allocation. Then we show that using 1-Level-Memory and the Device {DAX} mode by default is a good compromise. It may be easily used and partitioned for storage and memory-bound applications with locality awareness.},
	eventtitle = {Euro-Par 2019: Parallel Processing Workshops},
	author = {Goglin, Brice and Proaño, Andrès Rubio},
	urldate = {2021-02-10},
	date = {2019-08-26},
	langid = {english},
	file = {Full Text PDF:C\:\\Users\\A708881\\Zotero\\storage\\Z97K5V4M\\Goglin et Proaño - 2019 - Opportunities for Partitioning Non-Volatile Memory.pdf:application/pdf;Snapshot:C\:\\Users\\A708881\\Zotero\\storage\\9BUR4CHP\\hal-02173336.html:text/html},
}

@article{mason_unexpected_2020,
	title = {Unexpected Performance of Intel® Optane™ {DC} Persistent Memory},
	volume = {19},
	issn = {1556-6064},
	doi = {10.1109/LCA.2020.2987303},
	abstract = {We evaluated Intel® Optane™ {DC} Persistent Memory and found that Intel's persistent memory is highly sensitive to data locality, size, and access patterns, which becomes clearer by optimizing both virtual memory page size and data layout for locality. Using the Polybench high-performance computing benchmark suite and controlling for mapped page size, we evaluate persistent memory ({PMEM}) performance relative to {DRAM}. In particular, the Linux {PMEM} support maps preferentially maps persistent memory in large pages while always mapping {DRAM} to small pages. We observed using large pages for {PMEM} and small pages for {DRAM} can create a 5x difference in performance, dwarfing other effects discussed in the literature. We found {PMEM} performance comparable to {DRAM} performance for the majority of tests when controlled for page size and optimized for data locality.},
	pages = {55--58},
	number = {1},
	journaltitle = {{IEEE} Computer Architecture Letters},
	author = {Mason, T. and Doudali, T. D. and Seltzer, M. and Gavrilovska, A.},
	date = {2020-01},
	note = {Conference Name: {IEEE} Computer Architecture Letters},
	keywords = {Memory management, File systems, performance evaluation, {DRAM} chips, Random access memory, Resource management, Optimization, Linux, Standards, data layout, data locality, {DRAM} performance, Intel's persistent memory performance evaluation, Intel® Optane™ {DC} persistent memory performance evaluation, Linux {PMEM}, paged storage, Persistent memory, polybench high-performance computing benchmark suite, virtual memory page size mapping},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\A708881\\Zotero\\storage\\GFWJGIBV\\9072482.html:text/html},
}

@online{noauthor_best_nodate,
	title = {Best Practice Guide - Parallel I/O, February 2019},
	url = {https://prace-ri.eu/training-support/best-practice-guides/best-practice-guide-parallel-io/},
	abstract = {Back to Best Practice Guides {PDF} {DOWNLOAD} Version: 2.0 by 07-02-2019 Authors​ Sandra Mendez {LRZ}, Germany Sebastian Lührs {FZJ}, Germany Dominic Sloan-Murphy (Editor) {EPCC}, United Kingdom Andrew Turner (Editor) {EPCC}, United Kingdom Volker Weinberg (Editor) {LRZ}, Germany ​Back to Top Back to Best Practice Guides},
	titleaddon = {{PRACE}},
	urldate = {2021-02-18},
	langid = {american},
	file = {Snapshot:C\:\\Users\\A708881\\Zotero\\storage\\Y63ZHCWG\\best-practice-guide-parallel-io.html:text/html},
}

@inproceedings{liang_daos_2020,
	location = {Cham},
	title = {{DAOS}: A Scale-Out High Performance Storage Stack for Storage Class Memory},
	isbn = {978-3-030-48842-0},
	doi = {10.1007/978-3-030-48842-0_3},
	series = {Lecture Notes in Computer Science},
	shorttitle = {{DAOS}},
	abstract = {The Distributed Asynchronous Object Storage ({DAOS}) is an open source scale-out storage system that is designed from the ground up to support Storage Class Memory ({SCM}) and {NVMe} storage in user space. Its advanced storage {API} enables the native support of structured, semi-structured and unstructured data models, overcoming the limitations of traditional {POSIX} based parallel filesystem. For {HPC} workloads, {DAOS} provides direct {MPI}-{IO} and {HDF}5 support as well as {POSIX} access for legacy applications. In this paper we present the architecture of the {DAOS} storage engine and its high-level application interfaces. We also describe initial performance results of {DAOS} for {IO}500 benchmarks.},
	pages = {40--54},
	booktitle = {Supercomputing Frontiers},
	publisher = {Springer International Publishing},
	author = {Liang, Zhen and Lombardi, Johann and Chaarawi, Mohamad and Hennecke, Michael},
	editor = {Panda, Dhabaleswar K.},
	date = {2020},
	langid = {english},
	keywords = {Persistent memory, {DAOS}, Distributed storage system, {NVMe}, Parallel filesystem, {RAFT}, {SCM}, {SWIM}},
	file = {Springer Full Text PDF:C\:\\Users\\A708881\\Zotero\\storage\\K8ZEY6M9\\Liang et al. - 2020 - DAOS A Scale-Out High Performance Storage Stack f.pdf:application/pdf},
}

@inproceedings{wang_file_2020,
	location = {New York, {NY}, {USA}},
	title = {File System Semantics Requirements of {HPC} Applications},
	isbn = {978-1-4503-8217-5},
	url = {https://doi.org/10.1145/3431379.3460637},
	doi = {10.1145/3431379.3460637},
	series = {{HPDC} '21},
	abstract = {Most widely-deployed parallel file systems ({PFSs}) implement {POSIX} semantics, which implies sequential consistency for reads and writes. Strict adherence to {POSIX} semantics is known to impede performance and thus several new {PFSs} with relaxed consistency semantics and better performance have been introduced. Such {PFSs} are useful provided that applications can run correctly on a {PFS} with weaker semantics. While it is widely assumed that {HPC} applications do not require strict {POSIX} semantics, to our knowledge there has not been systematic work to support this assumption. In this paper, we address this gap with a categorization of the consistency semantics guarantees of {PFSs} and develop an algorithm to determine the consistency semantics requirements of a variety of {HPC} applications. We captured the I/O activity of 17 representative {HPC} applications and benchmarks as they performed I/O through {POSIX} or I/O libraries and examined the metadata operations used and their file access patterns. From this analysis, we find that 16 of the 17 applications can utilize {PFSs} with weaker semantics.},
	pages = {19--30},
	booktitle = {Proceedings of the 30th International Symposium on High-Performance Parallel and Distributed Computing},
	publisher = {Association for Computing Machinery},
	author = {Wang, Chen and Mohror, Kathryn and Snir, Marc},
	urldate = {2021-06-25},
	date = {2020-06-21},
	keywords = {consistency semantics, parallel file system, scientific applications},
	file = {Full Text PDF:C\:\\Users\\A708881\\Zotero\\storage\\BBKXNIQE\\Wang et al. - 2020 - File System Semantics Requirements of HPC Applicat.pdf:application/pdf},
}

@online{aaij_arxiv_2021,
	title = {{arXiv} : Evolution of the energy efficiency of {LHCb}'s real-time processing},
	url = {https://cds.cern.ch/record/2773126},
	shorttitle = {{arXiv}},
	abstract = {The upgraded {LHCb} detector, due to start datataking in 2022, will have to process an average data rate of 4{\textasciitilde}{TB}/s in real time. Because {LHCb}'s physics objectives require that the full detector information for every {LHC} bunch crossing is read out and made available for real-time processing, this bandwidth challenge is equivalent to that of the {ATLAS} and {CMS} {HL}-{LHC} software read-out, but deliverable five years earlier. Over the past six years, the {LHCb} collaboration has undertaken a bottom-up rewrite of its software infrastructure, pattern recognition, and selection algorithms to make them better able to efficiently exploit modern highly parallel computing architectures. We review the impact of this reoptimization on the energy efficiency of the real-time processing software and hardware which will be used for the upgrade of the {LHCb} detector. We also review the impact of the decision to adopt a hybrid computing architecture consisting of {GPUs} and {CPUs} for the real-time part of {LHCb}'s future data processing. We discuss the implications of these results on how {LHCb}'s real-time power requirements may evolve in the future, particularly in the context of a planned second upgrade of the detector.},
	titleaddon = {{CERN} Document Server},
	author = {Aaij, Roel and Colombo, Tommaso and Gligorov, Vladimir Vava and Schwemmer, Rainer and Hennequin, Arthur and Vom Bruch, Dorothea and Fitzpatrick, Conor and Neufeld, Niko and Cámpora Pérez, Daniel Hugo and Nolte, Niklas},
	urldate = {2021-06-28},
	date = {2021-06-14},
	langid = {french},
	note = {Number: {arXiv}:2106.07701},
	file = {Full Text PDF:C\:\\Users\\A708881\\Zotero\\storage\\BQMYFR9A\\Aaij et al. - 2021 - arXiv  Evolution of the energy efficiency of LHCb.pdf:application/pdf;Snapshot:C\:\\Users\\A708881\\Zotero\\storage\\KIR9G8IR\\2773126.html:text/html},
}

@inproceedings{rivas-gomez_ummap-io_2019,
	title = {{uMMAP}-{IO}: User-Level Memory-Mapped I/O for {HPC}},
	doi = {10.1109/HiPC.2019.00051},
	shorttitle = {{uMMAP}-{IO}},
	abstract = {The integration of local storage technologies alongside traditional parallel file systems on {HPC} clusters, is expected to rise the programming complexity on scientific applications aiming to take advantage of the increased-level of heterogeneity. In this work, we present {uMMAP}-{IO}, a user-level memory-mapped I/O implementation that simplifies data management on multi-tier storage subsystems. Compared to the memory-mapped I/O mechanism of the {OS}, our approach features per-allocation configurable settings (e.g., segment size) and transparently enables access to a diverse range of memory and storage technologies, such as the burst buffer I/O accelerators. Preliminary results indicate that {uMMAP}-{IO} provides at least 5-10x better performance on representative workloads in comparison with the standard memory-mapped I/O of the {OS}, and approximately 20-50\% degradation on average compared to using conventional memory allocations without storage support up to 8192 processes.},
	eventtitle = {2019 {IEEE} 26th International Conference on High Performance Computing, Data, and Analytics ({HiPC})},
	pages = {363--372},
	booktitle = {2019 {IEEE} 26th International Conference on High Performance Computing, Data, and Analytics ({HiPC})},
	author = {Rivas-Gomez, Sergio and Fanfarillo, Alessandro and Valat, Sebastien and Laferriere, Christophe and Couvee, Philippe and Narasimhamurthy, Sai and Markidis, Stefano},
	date = {2019-12},
	note = {{ISSN}: 2640-0316},
	keywords = {Libraries, Memory management, Random access memory, Resource management, Programming, Memory Mapped I/O, Parallel I/O, Supercomputers, Synchronization, {uMMAP} {IO}},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\A708881\\Zotero\\storage\\UAABNMF3\\8990626.html:text/html},
}

@inproceedings{nicolae_veloc_2019,
	title = {{VeloC}: Towards High Performance Adaptive Asynchronous Checkpointing at Large Scale},
	doi = {10.1109/IPDPS.2019.00099},
	shorttitle = {{VeloC}},
	abstract = {Global checkpointing to external storage (e.g., a parallel file system) is a common I/O pattern of many {HPC} applications. However, given the limited I/O throughput of external storage, global checkpointing can often lead to I/O bottlenecks. To address this issue, a shift from synchronous checkpointing (i.e., blocking until writes have finished) to asynchronous checkpointing (i.e., writing to faster local storage and flushing to external storage in the background) is increasingly being adopted. However, with rising core count per node and heterogeneity of both local and external storage, it is non-trivial to design efficient asynchronous checkpointing mechanisms due to the complex interplay between high concurrency and I/O performance variability at both the node-local and global levels. This problem is not well understood but highly important for modern supercomputing infrastructures. This paper proposes a versatile asynchronous checkpointing solution that addresses this problem. To this end, we introduce a concurrency-optimized technique that combines performance modeling with lightweight monitoring to make informed decisions about what local storage devices to use in order to dynamically adapt to background flushes and reduce the checkpointing overhead. We illustrate this technique using the {VeloC} prototype. Extensive experiments on a pre-Exascale supercomputing system show significant benefits.},
	eventtitle = {2019 {IEEE} International Parallel and Distributed Processing Symposium ({IPDPS})},
	pages = {911--920},
	booktitle = {2019 {IEEE} International Parallel and Distributed Processing Symposium ({IPDPS})},
	author = {Nicolae, Bogdan and Moody, Adam and Gonsiorowski, Elsa and Mohror, Kathryn and Cappello, Franck},
	date = {2019-05},
	note = {{ISSN}: 1530-2075},
	keywords = {Performance evaluation, Complexity theory, high performance computing, Bandwidth, checkpointing, Writing, Adaptation models, adaptive asynchronous I/O, Checkpointing, immutable data, parallel I/O, Runtime},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\A708881\\Zotero\\storage\\PVMFD3KH\\8821049.html:text/html;Version soumise:C\:\\Users\\A708881\\Zotero\\storage\\4SJVUI3B\\Nicolae et al. - 2019 - VeloC Towards High Performance Adaptive Asynchron.pdf:application/pdf},
}

@inproceedings{chien_sputnipic_2020,
	title = {{sputniPIC}: An Implicit Particle-in-Cell Code for Multi-{GPU} Systems},
	doi = {10.1109/SBAC-PAD49847.2020.00030},
	shorttitle = {{sputniPIC}},
	abstract = {Large-scale simulations of plasmas are essential for advancing our understanding of fusion devices, space, and astrophysical systems. Particle-in-Cell ({PIC}) codes have demonstrated their success in simulating numerous plasma phenomena on {HPC} systems. Today, flagship supercomputers feature multiple {GPUs} per compute node to achieve unprecedented computing power at high power efficiency. {PIC} codes require new algorithm design and implementation for exploiting such accelerated platforms. In this work, we design and optimize a three-dimensional implicit {PIC} code, called {sputniPIC}, to run on a general multi-{GPU} compute node. We introduce a particle decomposition data layout, in contrast to domain decomposition on {CPU}-based implementations, to use particle batches for overlapping communication and computation on {GPUs}. {sputniPIC} also natively supports different precision representations to achieve speed up on hardware that supports reduced precision. We validate {sputniPIC} through the well-known {GEM} challenge and provide performance analysis. We test {sputniPIC} on three multi-{GPU} platforms and report a 200-800x performance improvement with respect to the {sputniPIC} {CPU} {OpenMP} version performance. We show that reduced precision could further improve performance by 45\% to 80\% on the three platforms. Because of these performance improvements, on a single node with multiple {GPUs}, {sputniPIC} enables large-scale three-dimensional {PIC} simulations that were only possible using clusters.},
	eventtitle = {2020 {IEEE} 32nd International Symposium on Computer Architecture and High Performance Computing ({SBAC}-{PAD})},
	pages = {149--156},
	booktitle = {2020 {IEEE} 32nd International Symposium on Computer Architecture and High Performance Computing ({SBAC}-{PAD})},
	author = {Chien, Steven W. D. and Nylund, Jonas and Bengtsson, Gabriel and Peng, Ivy B. and Podobas, Artur and Markidis, Stefano},
	date = {2020-09},
	note = {{ISSN}: 2643-3001},
	keywords = {Performance evaluation, Supercomputers, Computational modeling, {CUDA}, Data models, Graphics processing units, implicit Particle-in-Cell, Interpolation, Mathematical model, multi-{GPU}, Nvidia {GPU}},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\A708881\\Zotero\\storage\\WYGBQXY5\\9235052.html:text/html;Version soumise:C\:\\Users\\A708881\\Zotero\\storage\\HFC49UHT\\Chien et al. - 2020 - sputniPIC An Implicit Particle-in-Cell Code for M.pdf:application/pdf},
}

@inproceedings{lu_madfs_2009,
	title = {{MADFS}: The Mobile Agent-Based Distributed Network File System},
	volume = {1},
	doi = {10.1109/GCIS.2009.208},
	shorttitle = {{MADFS}},
	abstract = {The conventional distributed file system is designed for {LAN} environment. They always play poor performance in {WAN}. In this paper we present a novel distributed file system: The Mobile Agent-based Distributed File System ({MADFS}). The objective of {MADFS} is to reduce the overhead of network transfer and cache management inherent to the distribution of a distributed files system in {WAN}. The {MADFS} organizes hosts into a hierarchical structure, and uses mobile agents as the underlying facility for transmission, communication and synchronization. We also present a novel cache coherency mechanism for {MADFS}: hierarchical and convergent cache coherency mechanism ({HCCM}). {HCCM} can effectively reduce the overhead of cache coherency management in distributed file system. In this paper, we also build an analysis model for {HCMM} and compare the performance to single-layer mode. The comparing results show that {HCMM} has better performance in {WAN}. In conclusion, {MADFS} can achieve better performance and availability than conventional distributed file system in {WAN}.},
	eventtitle = {2009 {WRI} Global Congress on Intelligent Systems},
	pages = {68--74},
	booktitle = {2009 {WRI} Global Congress on Intelligent Systems},
	author = {Lu, Jun and Du, Bin and Zhu, Yi and Li, {DaiWei}},
	date = {2009-05},
	note = {{ISSN}: 2155-6091},
	keywords = {File systems, Agent, Availability, Cache Coherenc, distributed file system, File servers, Local area networks, Mobile agents, Mobile communication, Network servers, Performance, Performance analysis, Software engineering, {WAN}, Wide area networks},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\A708881\\Zotero\\storage\\EH2YFWL7\\5209029.html:text/html},
}

@inproceedings{vef_gekkofs_2018,
	title = {{GekkoFS} - A Temporary Distributed File System for {HPC} Applications},
	doi = {10.1109/CLUSTER.2018.00049},
	abstract = {We present {GekkoFS}, a temporary, highly-scalable burst buffer file system which has been specifically optimized for new access patterns of data-intensive High-Performance Computing ({HPC}) applications. The file system provides relaxed {POSIX} semantics, only offering features which are actually required by most (not all) applications. It is able to provide scalable I/O performance and reaches millions of metadata operations already for a small number of nodes, significantly outperforming the capabilities of general-purpose parallel file systems.},
	eventtitle = {2018 {IEEE} International Conference on Cluster Computing ({CLUSTER})},
	pages = {319--324},
	booktitle = {2018 {IEEE} International Conference on Cluster Computing ({CLUSTER})},
	author = {Vef, Marc-André and Moti, Nafiseh and Süß, Tim and Tocci, Tommaso and Nou, Ramon and Miranda, Alberto and Cortes, Toni and Brinkmann, André},
	date = {2018-09},
	note = {{ISSN}: 2168-9253},
	keywords = {{HPC}, Libraries, Servers, Metadata, Semantics, Burst Buffers, Data structures, Distributed File Systems, Systems operation, Throughput},
	file = {Texte intégral:C\:\\Users\\A708881\\Zotero\\storage\\BAF823K6\\Vef et al. - 2018 - GekkoFS - A Temporary Distributed File System for .pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\A708881\\Zotero\\storage\\IQVKA55A\\8514892.html:text/html},
}

@inproceedings{wang_ephemeral_2016,
	title = {An Ephemeral Burst-Buffer File System for Scientific Applications},
	doi = {10.1109/SC.2016.68},
	abstract = {Burst buffers are becoming an indispensable hardware resource on large-scale supercomputers to buffer the bursty I/O from scientific applications. However, there is a lack of software support for burst buffers to be efficiently shared by applications within a batch-submitted job and recycled across different batch jobs. In addition, burst buffers need to cope with a variety of challenging I/O patterns from data-intensive scientific applications. In this study, we have designed an ephemeral Burst Buffer File System ({BurstFS}) that supports scalable and efficient aggregation of I/O bandwidth from burst buffers while having the same life cycle as a batch-submitted job. {BurstFS} features several techniques including scalable metadata indexing, co-located I/O delegation, and server-side read clustering and pipelining. Through extensive tuning and analysis, we have validated that {BurstFS} has accomplished our design objectives, with linear scalability in terms of aggregated I/O bandwidth for parallel writes and reads.},
	eventtitle = {{SC} '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
	pages = {807--818},
	booktitle = {{SC} '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
	author = {Wang, Teng and Mohror, Kathryn and Moody, Adam and Sato, Kento and Yu, Weikuan},
	date = {2016-11},
	note = {{ISSN}: 2167-4337},
	keywords = {Pipeline processing, File systems, Metadata, Bandwidth, Buffer storage, Distributed databases, Indexing},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\A708881\\Zotero\\storage\\8LWJX5UG\\7877147.html:text/html},
}

@inproceedings{serebryany_addresssanitizer_2012,
	location = {{USA}},
	title = {{AddressSanitizer}: a fast address sanity checker},
	series = {{USENIX} {ATC}'12},
	shorttitle = {{AddressSanitizer}},
	abstract = {Memory access bugs, including buffer overflows and uses of freed heap memory, remain a serious problem for programming languages like C and C++. Many memory error detectors exist, but most of them are either slow or detect a limited set of bugs, or both. This paper presents {AddressSanitizer}, a new memory error detector. Our tool finds out-of-bounds accesses to heap, stack, and global objects, as well as use-after-free bugs. It employs a specialized memory allocator and code instrumentation that is simple enough to be implemented in any compiler, binary translation system, or even in hardware. {AddressSanitizer} achieves efficiency without sacrificing comprehensiveness. Its average slowdown is just 73\% yet it accurately detects bugs at the point of occurrence. It has found over 300 previously unknown bugs in the Chromium browser and many bugs in other software.},
	pages = {28},
	booktitle = {Proceedings of the 2012 {USENIX} conference on Annual Technical Conference},
	publisher = {{USENIX} Association},
	author = {Serebryany, Konstantin and Bruening, Derek and Potapenko, Alexander and Vyukov, Dmitry},
	urldate = {2021-09-16},
	date = {2012-06-13},
}
